---
task_id: 002
title: API endpoints for wizard interactions
type: backend
status: pending
priority: high
effort: 20
parallel: true
depends_on: [001]
created: 2025-08-21T20:56:15Z
updated: 2025-08-21T20:56:15Z
assignee: null
github_issue: null
---

# Task #002: API endpoints for wizard interactions

**Epic**: ml-experiment-form-improvements  
**Status**: ðŸ”„ PENDING  
**Duration**: 20 hours (2.5 days)  
**Type**: Backend Foundation  

## ðŸŽ¯ Objective

Create comprehensive API endpoints to support the multi-step experiment wizard, including dataset analysis, model recommendations, real-time validation, and template management. These endpoints will provide the backend infrastructure for an intelligent, guided experiment creation experience.

## ðŸ“‹ Requirements

### Primary Deliverables

1. **Wizard Step APIs**
   - Dataset analysis and preview endpoints
   - Model recommendation engine endpoints
   - Real-time configuration validation
   - Wizard state management APIs

2. **Template Management APIs**
   - CRUD operations for experiment templates
   - Template search and filtering
   - Template application and inheritance
   - Template sharing and permissions

3. **Validation and Feedback APIs**
   - Parameter validation with conflict detection
   - Resource estimation endpoints
   - Configuration compatibility checks
   - Progress tracking and state persistence

## ðŸ”§ Technical Implementation

### API Structure and Endpoints

#### Wizard Core APIs
```python
# views/wizard.py
from rest_framework import status, permissions
from rest_framework.decorators import api_view, permission_classes
from rest_framework.response import Response

@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def analyze_dataset(request):
    """
    POST /api/experiments/wizard/dataset-analysis/
    Analyze uploaded dataset and provide statistics, recommendations
    """
    
@api_view(['GET'])
@permission_classes([permissions.IsAuthenticated])
def get_model_recommendations(request):
    """
    GET /api/experiments/wizard/model-recommendations/
    Get intelligent model recommendations based on dataset characteristics
    """
    
@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def calculate_feature_importance(request):
    """
    POST /api/experiments/wizard/feature-importance/
    Calculate feature importance and correlation analysis
    """
    
@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def validate_configuration(request):
    """
    POST /api/experiments/wizard/validate-config/
    Real-time validation of experiment configuration
    """
    
@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def estimate_resources(request):
    """
    POST /api/experiments/wizard/resource-estimation/
    Estimate training time and computational requirements
    """
```

#### Template Management APIs
```python
# views/templates.py
from rest_framework.viewsets import ModelViewSet
from rest_framework.decorators import action
from .models import ExperimentTemplate
from .serializers import ExperimentTemplateSerializer

class ExperimentTemplateViewSet(ModelViewSet):
    """
    ViewSet for experiment template CRUD operations
    
    Endpoints:
    GET /api/experiment-templates/ - List templates
    POST /api/experiment-templates/ - Create template
    GET /api/experiment-templates/{id}/ - Retrieve template
    PUT /api/experiment-templates/{id}/ - Update template
    DELETE /api/experiment-templates/{id}/ - Delete template
    """
    serializer_class = ExperimentTemplateSerializer
    permission_classes = [permissions.IsAuthenticated]
    
    def get_queryset(self):
        """Filter templates based on user permissions"""
        user = self.request.user
        return ExperimentTemplate.objects.filter(
            Q(created_by=user) | Q(is_public=True) | Q(shared_with=user)
        ).distinct()
    
    @action(detail=True, methods=['post'])
    def apply(self, request, pk=None):
        """
        POST /api/experiment-templates/{id}/apply/
        Apply template to create new experiment configuration
        """
        template = self.get_object()
        # Implementation details...
        
    @action(detail=False, methods=['get'])
    def categories(self, request):
        """
        GET /api/experiment-templates/categories/
        Get available template categories
        """
        
    @action(detail=False, methods=['get'])
    def popular(self, request):
        """
        GET /api/experiment-templates/popular/
        Get most popular templates based on usage
        """
```

#### Wizard State Management
```python
# views/wizard_state.py
@api_view(['GET', 'POST', 'DELETE'])
@permission_classes([permissions.IsAuthenticated])
def wizard_state(request, experiment_id=None):
    """
    GET /api/experiments/wizard/state/{experiment_id}/ - Get wizard state
    POST /api/experiments/wizard/state/{experiment_id}/ - Save wizard state
    DELETE /api/experiments/wizard/state/{experiment_id}/ - Clear wizard state
    """
    
@api_view(['POST'])
@permission_classes([permissions.IsAuthenticated])
def save_draft(request):
    """
    POST /api/experiments/wizard/save-draft/
    Save experiment draft with current wizard state
    """
    
@api_view(['GET'])
@permission_classes([permissions.IsAuthenticated])
def get_drafts(request):
    """
    GET /api/experiments/wizard/drafts/
    Get user's experiment drafts
    """
```

### Serializers and Data Validation

#### Dataset Analysis Serializer
```python
# serializers/wizard.py
from rest_framework import serializers

class DatasetAnalysisSerializer(serializers.Serializer):
    dataset_file = serializers.FileField()
    target_column = serializers.CharField(required=False)
    sample_size = serializers.IntegerField(default=1000, min_value=100, max_value=10000)
    
    def validate_dataset_file(self, value):
        """Validate file type and size"""
        if not value.name.endswith(('.csv', '.xlsx', '.json', '.parquet')):
            raise serializers.ValidationError("Unsupported file format")
        
        if value.size > 50 * 1024 * 1024:  # 50MB limit
            raise serializers.ValidationError("File too large")
        
        return value

class ModelRecommendationSerializer(serializers.Serializer):
    dataset_stats = serializers.JSONField()
    problem_type = serializers.ChoiceField(
        choices=[('classification', 'Classification'), ('regression', 'Regression')]
    )
    performance_priority = serializers.ChoiceField(
        choices=[('accuracy', 'Accuracy'), ('speed', 'Speed'), ('interpretability', 'Interpretability')],
        default='accuracy'
    )

class ConfigurationValidationSerializer(serializers.Serializer):
    configuration = serializers.JSONField()
    step = serializers.CharField()
    
    def validate_configuration(self, value):
        """Validate configuration against schema"""
        from .validators import validate_experiment_configuration
        is_valid, error = validate_experiment_configuration(value)
        if not is_valid:
            raise serializers.ValidationError(f"Configuration invalid: {error}")
        return value
```

#### Template Serializers
```python
# serializers/templates.py
class ExperimentTemplateSerializer(serializers.ModelSerializer):
    created_by_username = serializers.CharField(source='created_by.username', read_only=True)
    usage_count = serializers.IntegerField(read_only=True)
    can_edit = serializers.SerializerMethodField()
    
    class Meta:
        model = ExperimentTemplate
        fields = [
            'id', 'name', 'description', 'category', 'tags',
            'configuration', 'version', 'is_active', 'is_public',
            'created_by_username', 'usage_count', 'last_used',
            'created_at', 'updated_at', 'can_edit'
        ]
        read_only_fields = ['created_by', 'version', 'usage_count', 'last_used']
    
    def get_can_edit(self, obj):
        """Check if current user can edit this template"""
        request = self.context.get('request')
        if not request:
            return False
        return obj.created_by == request.user
    
    def create(self, validated_data):
        """Set created_by to current user"""
        validated_data['created_by'] = self.context['request'].user
        return super().create(validated_data)

class TemplateApplicationSerializer(serializers.Serializer):
    template_id = serializers.UUIDField()
    overrides = serializers.JSONField(default=dict)
    experiment_name = serializers.CharField(max_length=255)
```

### Service Layer Implementation

#### Dataset Analysis Service
```python
# services/dataset_analysis.py
import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
from typing import Dict, Any, Tuple

class DatasetAnalysisService:
    """Service for analyzing datasets and providing insights"""
    
    @staticmethod
    def analyze_dataset(file_path: str, target_column: str = None, sample_size: int = 1000) -> Dict[str, Any]:
        """
        Comprehensive dataset analysis
        Returns statistics, feature insights, and recommendations
        """
        try:
            # Load dataset with appropriate method
            df = DatasetAnalysisService._load_dataset(file_path)
            
            # Sample for performance if dataset is large
            if len(df) > sample_size:
                df = df.sample(n=sample_size, random_state=42)
            
            analysis = {
                'basic_stats': DatasetAnalysisService._get_basic_stats(df),
                'feature_analysis': DatasetAnalysisService._analyze_features(df),
                'data_quality': DatasetAnalysisService._assess_data_quality(df),
                'recommendations': DatasetAnalysisService._generate_recommendations(df, target_column)
            }
            
            if target_column and target_column in df.columns:
                analysis['target_analysis'] = DatasetAnalysisService._analyze_target(df, target_column)
                analysis['feature_importance'] = DatasetAnalysisService._calculate_feature_importance(df, target_column)
            
            return analysis
            
        except Exception as e:
            raise ValueError(f"Dataset analysis failed: {str(e)}")
    
    @staticmethod
    def _load_dataset(file_path: str) -> pd.DataFrame:
        """Load dataset based on file extension"""
        if file_path.endswith('.csv'):
            return pd.read_csv(file_path)
        elif file_path.endswith('.xlsx'):
            return pd.read_excel(file_path)
        elif file_path.endswith('.json'):
            return pd.read_json(file_path)
        elif file_path.endswith('.parquet'):
            return pd.read_parquet(file_path)
        else:
            raise ValueError(f"Unsupported file format: {file_path}")
    
    @staticmethod
    def _get_basic_stats(df: pd.DataFrame) -> Dict[str, Any]:
        """Get basic dataset statistics"""
        return {
            'rows': len(df),
            'columns': len(df.columns),
            'memory_usage': df.memory_usage(deep=True).sum(),
            'column_types': df.dtypes.value_counts().to_dict(),
            'missing_values': df.isnull().sum().to_dict()
        }
    
    @staticmethod
    def _analyze_features(df: pd.DataFrame) -> Dict[str, Any]:
        """Analyze individual features"""
        numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        
        return {
            'numerical_features': {
                'count': len(numerical_cols),
                'columns': numerical_cols,
                'statistics': df[numerical_cols].describe().to_dict() if numerical_cols else {}
            },
            'categorical_features': {
                'count': len(categorical_cols),
                'columns': categorical_cols,
                'cardinality': {col: df[col].nunique() for col in categorical_cols}
            }
        }
```

#### Model Recommendation Service
```python
# services/model_recommendations.py
class ModelRecommendationService:
    """Service for recommending models based on dataset characteristics"""
    
    MODEL_RECOMMENDATIONS = {
        'classification': {
            'small_dataset': ['random_forest', 'svm', 'logistic_regression'],
            'large_dataset': ['gradient_boosting', 'neural_network', 'random_forest'],
            'high_dimensional': ['regularized_linear', 'random_forest', 'feature_selection_pipeline'],
            'interpretable': ['decision_tree', 'logistic_regression', 'naive_bayes']
        },
        'regression': {
            'small_dataset': ['random_forest', 'svr', 'linear_regression'],
            'large_dataset': ['gradient_boosting', 'neural_network', 'random_forest'],
            'high_dimensional': ['ridge_regression', 'lasso_regression', 'elastic_net'],
            'interpretable': ['linear_regression', 'decision_tree', 'polynomial_features']
        }
    }
    
    @staticmethod
    def get_recommendations(dataset_stats: Dict[str, Any], problem_type: str, priority: str = 'accuracy') -> Dict[str, Any]:
        """Get model recommendations based on dataset characteristics"""
        
        rows = dataset_stats.get('basic_stats', {}).get('rows', 0)
        columns = dataset_stats.get('basic_stats', {}).get('columns', 0)
        
        # Determine dataset category
        dataset_category = ModelRecommendationService._categorize_dataset(rows, columns)
        
        # Get base recommendations
        base_recommendations = ModelRecommendationService.MODEL_RECOMMENDATIONS.get(problem_type, {})
        recommended_models = base_recommendations.get(dataset_category, [])
        
        # Adjust based on priority
        if priority == 'interpretability':
            recommended_models = base_recommendations.get('interpretable', recommended_models)
        
        # Generate detailed recommendations
        detailed_recommendations = []
        for model in recommended_models[:3]:  # Top 3 recommendations
            recommendation = ModelRecommendationService._get_model_details(model, dataset_stats, priority)
            detailed_recommendations.append(recommendation)
        
        return {
            'problem_type': problem_type,
            'dataset_category': dataset_category,
            'priority': priority,
            'recommendations': detailed_recommendations,
            'reasoning': ModelRecommendationService._generate_reasoning(dataset_stats, recommended_models)
        }
    
    @staticmethod
    def _categorize_dataset(rows: int, columns: int) -> str:
        """Categorize dataset size"""
        if rows < 1000:
            return 'small_dataset'
        elif columns > rows * 0.1:  # High dimensional
            return 'high_dimensional'
        else:
            return 'large_dataset'
```

### API Response Formats

#### Dataset Analysis Response
```json
{
    "success": true,
    "data": {
        "basic_stats": {
            "rows": 1000,
            "columns": 15,
            "memory_usage": 120000,
            "column_types": {"int64": 5, "float64": 8, "object": 2},
            "missing_values": {"col1": 0, "col2": 5}
        },
        "feature_analysis": {
            "numerical_features": {
                "count": 13,
                "columns": ["age", "income", "score"],
                "statistics": {}
            },
            "categorical_features": {
                "count": 2,
                "columns": ["category", "region"],
                "cardinality": {"category": 5, "region": 3}
            }
        },
        "recommendations": {
            "suggested_target": "score",
            "problem_type": "regression",
            "preprocessing_steps": ["handle_missing", "scale_features"]
        }
    }
}
```

#### Model Recommendations Response
```json
{
    "success": true,
    "data": {
        "problem_type": "classification",
        "dataset_category": "large_dataset",
        "priority": "accuracy",
        "recommendations": [
            {
                "model": "gradient_boosting",
                "name": "Gradient Boosting Classifier",
                "confidence": 0.95,
                "pros": ["High accuracy", "Handles mixed data types"],
                "cons": ["Longer training time", "Less interpretable"],
                "parameters": {
                    "n_estimators": 100,
                    "learning_rate": 0.1,
                    "max_depth": 6
                }
            }
        ],
        "reasoning": "Based on your dataset size (10,000 rows) and mixed feature types..."
    }
}
```

## ðŸ§ª Testing Requirements

### API Tests
```python
# tests/test_wizard_apis.py
from django.test import TestCase
from django.urls import reverse
from rest_framework.test import APIClient
from rest_framework import status

class WizardAPITest(TestCase):
    def setUp(self):
        self.client = APIClient()
        self.user = User.objects.create_user(username='testuser', password='testpass')
        self.client.force_authenticate(user=self.user)
    
    def test_dataset_analysis_api(self):
        """Test dataset analysis endpoint"""
        with open('test_data.csv', 'rb') as f:
            response = self.client.post(
                reverse('wizard:analyze-dataset'),
                {'dataset_file': f, 'sample_size': 500},
                format='multipart'
            )
        
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertIn('basic_stats', response.data['data'])
    
    def test_model_recommendations_api(self):
        """Test model recommendations endpoint"""
        data = {
            'dataset_stats': {'basic_stats': {'rows': 1000, 'columns': 10}},
            'problem_type': 'classification',
            'performance_priority': 'accuracy'
        }
        
        response = self.client.get(reverse('wizard:model-recommendations'), data)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertIn('recommendations', response.data['data'])
    
    def test_configuration_validation(self):
        """Test real-time configuration validation"""
        config = {
            'dataset': {'file': 'test.csv'},
            'features': ['col1', 'col2'],
            'model': {'type': 'random_forest'}
        }
        
        response = self.client.post(
            reverse('wizard:validate-config'),
            {'configuration': config, 'step': 'model_selection'}
        )
        
        self.assertEqual(response.status_code, status.HTTP_200_OK)
```

### Service Tests
```python
# tests/test_services.py
class DatasetAnalysisServiceTest(TestCase):
    def test_analyze_csv_dataset(self):
        """Test CSV dataset analysis"""
        analysis = DatasetAnalysisService.analyze_dataset('test_data.csv')
        
        self.assertIn('basic_stats', analysis)
        self.assertIn('feature_analysis', analysis)
        self.assertIn('recommendations', analysis)
    
    def test_feature_importance_calculation(self):
        """Test feature importance calculation"""
        # Implementation details...
        
class ModelRecommendationServiceTest(TestCase):
    def test_classification_recommendations(self):
        """Test classification model recommendations"""
        dataset_stats = {'basic_stats': {'rows': 5000, 'columns': 20}}
        recommendations = ModelRecommendationService.get_recommendations(
            dataset_stats, 'classification', 'accuracy'
        )
        
        self.assertEqual(recommendations['problem_type'], 'classification')
        self.assertGreater(len(recommendations['recommendations']), 0)
```

## ðŸ“Š Performance Requirements

### Response Time Targets
- **Dataset Analysis**: < 2 seconds for files up to 50MB
- **Model Recommendations**: < 500ms for typical requests
- **Configuration Validation**: < 100ms for real-time validation
- **Template Operations**: < 200ms for CRUD operations

### Optimization Strategies
```python
# Caching for expensive operations
from django.core.cache import cache

def get_model_recommendations(dataset_stats, problem_type, priority):
    cache_key = f"recommendations_{hash(str(dataset_stats))}_{problem_type}_{priority}"
    cached_result = cache.get(cache_key)
    
    if cached_result:
        return cached_result
    
    result = ModelRecommendationService.get_recommendations(dataset_stats, problem_type, priority)
    cache.set(cache_key, result, timeout=3600)  # Cache for 1 hour
    return result
```

## ðŸ” Acceptance Criteria

### Functional Requirements
- [ ] All wizard API endpoints respond with correct data formats
- [ ] Dataset analysis provides comprehensive statistics and insights
- [ ] Model recommendations are relevant and well-reasoned
- [ ] Configuration validation catches common errors and conflicts
- [ ] Template CRUD operations work correctly with permissions
- [ ] Wizard state persistence maintains data across sessions

### Performance Requirements
- [ ] Dataset analysis completes within 2 seconds for 50MB files
- [ ] API responses are under 500ms for cached operations
- [ ] Real-time validation responds within 100ms
- [ ] Template operations complete within 200ms

### Security Requirements
- [ ] All endpoints require authentication
- [ ] File upload validation prevents malicious files
- [ ] Template permissions properly restrict access
- [ ] Input validation prevents injection attacks

## ðŸš€ Implementation Plan

### Phase 1: Core Wizard APIs (8 hours)
1. Implement dataset analysis endpoint
2. Create model recommendation endpoint
3. Build configuration validation endpoint
4. Add resource estimation endpoint

### Phase 2: Template Management APIs (6 hours)
1. Create template CRUD ViewSet
2. Implement template application endpoint
3. Add template search and filtering
4. Build permission system

### Phase 3: State Management APIs (4 hours)
1. Implement wizard state persistence
2. Create draft save/load functionality
3. Add progress tracking endpoints
4. Build cleanup mechanisms

### Phase 4: Testing and Documentation (2 hours)
1. Write comprehensive API tests
2. Create service layer tests
3. Generate API documentation
4. Performance testing and optimization

## ðŸ”— Dependencies

### Prerequisites
- **Task #001**: Database models must be implemented first
- Django REST Framework configured
- File upload handling configured
- Authentication system in place

### Integration Points
- **Task #003**: Dataset analysis service will use these APIs
- **Frontend Tasks**: Wizard components will consume these endpoints
- **Template System**: Template management APIs support template workflow

## ðŸ“ Notes

### API Design Decisions
- **RESTful Endpoints**: Follow REST conventions for consistency
- **JSON Responses**: Standardized response format with success/error handling
- **File Upload Support**: Multipart form data for dataset uploads
- **Real-time Validation**: Optimized for frequent validation requests

### Error Handling Strategy
- Comprehensive error messages for debugging
- Graceful degradation when services are unavailable
- Input validation at multiple layers
- Proper HTTP status codes for different error types

### Future Extensibility
- Plugin architecture for additional analysis methods
- Configurable recommendation algorithms
- Extensible validation rule system
- Template sharing and marketplace features